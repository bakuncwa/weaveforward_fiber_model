{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9cc9d62",
   "metadata": {},
   "source": [
    "# WeaveForward — Web Scraper Extraction\n",
    "\n",
    "**Purpose:** Scrape fiber-composition and clothing-type data directly from the\n",
    "product pages of **25 sample Philippine clothing brands** and **25 sample multinational\n",
    "fashion retailers**, parse fiber `%` values, compute EU Ecodesign\n",
    "biodegradability tiers, and build a `BRAND_FIBER_LOOKUP` table.\n",
    "\n",
    "**Outputs written to `data/webscraped_data/`:**\n",
    "- `YYYYMMDD-HHMMSS-webscraped_catalog.csv` — timestamped full product catalog\n",
    "- `webscraped_catalog.csv` — stable alias (always the latest run)\n",
    "\n",
    "**Outputs written to `data/processed/`:**\n",
    "- `YYYYMMDD-HHMMSS-brand_fiber_lookup.json` — timestamped brand fiber profile\n",
    "- `brand_fiber_lookup.json` — stable alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42122219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ environment ready\n",
      "  ROOT    : /Users/gyalm/Desktop/WeaveForward_FiberClassificationML\n",
      "  WEB_DIR : /Users/gyalm/Desktop/WeaveForward_FiberClassificationML/data/webscraped_data\n",
      "  PROC_DIR: /Users/gyalm/Desktop/WeaveForward_FiberClassificationML/data/processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "from pathlib import Path\n",
    "\n",
    "PACKAGES = [\n",
    "    \"requests\",\n",
    "    \"beautifulsoup4\",\n",
    "    \"lxml\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"tqdm\",\n",
    "]\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *PACKAGES])\n",
    "\n",
    "# ── Path definitions ──────────────────────────────────────────────────────\n",
    "ROOT          = Path(\"..\").resolve()\n",
    "DATA_DIR      = ROOT / \"data\"\n",
    "PROC_DIR      = DATA_DIR / \"processed\"\n",
    "WEB_DIR       = DATA_DIR / \"webscraped_data\"\n",
    "\n",
    "for d in [PROC_DIR, WEB_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✓ environment ready\")\n",
    "print(f\"  ROOT    : {ROOT}\")\n",
    "print(f\"  WEB_DIR : {WEB_DIR}\")\n",
    "print(f\"  PROC_DIR: {PROC_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eddf2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ imports ready\n"
     ]
    }
   ],
   "source": [
    "import json, re, time, random, datetime\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4         import BeautifulSoup\n",
    "from tqdm        import tqdm\n",
    "from pathlib     import Path\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "SCRAPE_TIMEOUT  = 12   # seconds per request\n",
    "SCRAPE_DELAY    = 1.2  # polite delay between requests\n",
    "\n",
    "print(\"✓ imports ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad6af62",
   "metadata": {},
   "source": [
    "---\n",
    "## 1-A · Brand Catalogue — Philippine & Multinational Retailers\n",
    "\n",
    "Each brand entry holds only the **root / homepage URL**.  \n",
    "`discover_collection_urls()` (1-B-3) crawls that root to find all clothing-category\n",
    "and collection sub-pages automatically, so no paths are hardcoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c039f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 25 Philippine brands + 25 multinational = 50 total\n"
     ]
    }
   ],
   "source": [
    "# ── 1-A  Brand catalogue ──────────────────────────────────────────────────\n",
    "# Each entry: (brand_name, root_url, source_region, country)\n",
    "# root_url is the brand homepage or top-level shop root.\n",
    "# Collection sub-pages are discovered automatically by discover_collection_urls().\n",
    "\n",
    "PH_BRANDS = [\n",
    "    (\"Bench\",              \"https://www.bench.com.ph\",               \"philippine\", \"PH\"),\n",
    "    (\"Plains & Prints\",    \"https://www.plainsandprints.com\",        \"philippine\", \"PH\"),\n",
    "    (\"Penshoppe\",          \"https://www.penshoppe.com\",              \"philippine\", \"PH\"),\n",
    "    (\"Folded & Hung\",      \"https://www.foldedandhung.com.ph\",       \"philippine\", \"PH\"),\n",
    "    (\"Bayo\",               \"https://www.bayo.com.ph\",                \"philippine\", \"PH\"),\n",
    "    (\"Kashieca\",           \"https://www.kashieca.com\",               \"philippine\", \"PH\"),\n",
    "    (\"Human\",              \"https://www.human.com.ph\",               \"philippine\", \"PH\"),\n",
    "    (\"Oxygen\",             \"https://www.oxygenonline.com.ph\",        \"philippine\", \"PH\"),\n",
    "    (\"Something Borrowed\", \"https://www.somethingborrowed.ph\",       \"philippine\", \"PH\"),\n",
    "    (\"Artwork Studio\",     \"https://www.artworkstudio.com.ph\",       \"philippine\", \"PH\"),\n",
    "    (\"Regatta\",            \"https://www.regatta.com.ph\",             \"philippine\", \"PH\"),\n",
    "    (\"Caramelo\",           \"https://www.caramelo.ph\",                \"philippine\", \"PH\"),\n",
    "    (\"Dimensione\",         \"https://www.dimensione.com.ph\",          \"philippine\", \"PH\"),\n",
    "    (\"Hnos\",               \"https://www.hnos.ph\",                    \"philippine\", \"PH\"),\n",
    "    (\"Island Souvenirs\",   \"https://www.islandsouvenirs.com.ph\",     \"philippine\", \"PH\"),\n",
    "    (\"Freego\",             \"https://www.freego.com.ph\",              \"philippine\", \"PH\"),\n",
    "    (\"Tyler Fashion\",      \"https://www.tylerfashion.com.ph\",        \"philippine\", \"PH\"),\n",
    "    (\"Tomato\",             \"https://www.tomato.ph\",                  \"philippine\", \"PH\"),\n",
    "    (\"Love Bonito PH\",     \"https://lovebonito.com\",                 \"philippine\", \"PH\"),\n",
    "    (\"Giordano PH\",        \"https://www.giordano.com/ph\",            \"philippine\", \"PH\"),\n",
    "    (\"Muji PH\",            \"https://www.muji.com/ph\",                \"philippine\", \"PH\"),\n",
    "    (\"Koton PH\",           \"https://www.koton.com/ph/en\",            \"philippine\", \"PH\"),\n",
    "    (\"Terranova PH\",       \"https://www.terranovastyle.com/ph/en\",   \"philippine\", \"PH\"),\n",
    "    (\"The Varsity PH\",     \"https://www.thevarsity.ph\",              \"philippine\", \"PH\"),\n",
    "    (\"Stradivarius PH\",    \"https://www.stradivarius.com/ph\",        \"philippine\", \"PH\"),\n",
    "]\n",
    "\n",
    "MULTINATIONAL_BRANDS = [\n",
    "    (\"H&M\",              \"https://www2.hm.com/en_ph\",                \"multinational\", \"SE\"),\n",
    "    (\"Zara\",             \"https://www.zara.com/ph/en\",               \"multinational\", \"ES\"),\n",
    "    (\"Uniqlo\",           \"https://www.uniqlo.com/ph/en\",             \"multinational\", \"JP\"),\n",
    "    (\"Nike\",             \"https://www.nike.com/ph\",                  \"multinational\", \"US\"),\n",
    "    (\"Adidas\",           \"https://www.adidas.com.ph\",                \"multinational\", \"DE\"),\n",
    "    (\"Levi's\",           \"https://www.levi.com/PH/en_PH\",           \"multinational\", \"US\"),\n",
    "    (\"Mango\",            \"https://shop.mango.com/ph\",                \"multinational\", \"ES\"),\n",
    "    (\"Pull&Bear\",        \"https://www.pullandbear.com/ph\",           \"multinational\", \"ES\"),\n",
    "    (\"Bershka\",          \"https://www.bershka.com/ph\",               \"multinational\", \"ES\"),\n",
    "    (\"Massimo Dutti\",    \"https://www.massimodutti.com/ph\",          \"multinational\", \"ES\"),\n",
    "    (\"Tommy Hilfiger\",   \"https://www.tommy.com/en\",                 \"multinational\", \"US\"),\n",
    "    (\"Calvin Klein\",     \"https://www.calvinklein.us\",               \"multinational\", \"US\"),\n",
    "    (\"Lacoste\",          \"https://www.lacoste.com/ph\",               \"multinational\", \"FR\"),\n",
    "    (\"Guess\",            \"https://www.guess.com/en\",                 \"multinational\", \"US\"),\n",
    "    (\"Lululemon\",        \"https://www.lululemon.com\",                \"multinational\", \"CA\"),\n",
    "    (\"Patagonia\",        \"https://www.patagonia.com\",                \"multinational\", \"US\"),\n",
    "    (\"The North Face\",   \"https://www.thenorthface.com/en-us\",       \"multinational\", \"US\"),\n",
    "    (\"Under Armour\",     \"https://www.underarmour.com/en-ph\",        \"multinational\", \"US\"),\n",
    "    (\"Puma\",             \"https://ph.puma.com/ph/en\",                \"multinational\", \"DE\"),\n",
    "    (\"New Balance\",      \"https://www.newbalance.com.ph\",            \"multinational\", \"US\"),\n",
    "    (\"Columbia\",         \"https://www.columbia.com\",                 \"multinational\", \"US\"),\n",
    "    (\"Everlane\",         \"https://www.everlane.com\",                 \"multinational\", \"US\"),\n",
    "    (\"Gap\",              \"https://www.gap.com\",                      \"multinational\", \"US\"),\n",
    "    (\"Ralph Lauren\",     \"https://www.ralphlauren.com\",              \"multinational\", \"US\"),\n",
    "    (\"Banana Republic\",  \"https://bananarepublic.gap.com\",           \"multinational\", \"US\"),\n",
    "]\n",
    "\n",
    "ALL_BRANDS = PH_BRANDS + MULTINATIONAL_BRANDS\n",
    "print(f\"✓ {len(PH_BRANDS)} Philippine brands + {len(MULTINATIONAL_BRANDS)} multinational = {len(ALL_BRANDS)} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82cd401",
   "metadata": {},
   "source": [
    "---\n",
    "## 1-B · Live Scraper — requests + BeautifulSoup\n",
    "\n",
    "### 1-B-1 · Fiber Pattern Parser & Clothing-Type Detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48a0d262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ FIBER_RE and CLOTHING_TYPE_MAP ready\n"
     ]
    }
   ],
   "source": [
    "# ── Regex & mapping constants ─────────────────────────────────────────────\n",
    "# Fiber composition regex — matches patterns like \"95% Cotton\" or \"100% Polyester\"\n",
    "FIBER_RE = re.compile(\n",
    "    r'(\\d{1,3})\\s*%\\s*(cotton|polyester|nylon|wool|linen|silk|rayon|viscose|'\n",
    "    r'acrylic|elastane|spandex|lycra|modal|bamboo|hemp|denim|cashmere|tencel|'\n",
    "    r'lyocell|polyamide|polypropylene|microfibre|microfiber|fleece|acetate|'\n",
    "    r'angora|mohair|alpaca)',\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# Keyword → canonical clothing category mapping\n",
    "CLOTHING_TYPE_MAP = {\n",
    "    \"dress\": \"dress\",       \"dresses\": \"dress\",\n",
    "    \"top\": \"top\",           \"tops\": \"top\",\n",
    "    \"shirt\": \"shirt\",       \"shirts\": \"shirt\",      \"blouse\": \"shirt\",\n",
    "    \"tee\": \"t-shirt\",       \"t-shirt\": \"t-shirt\",   \"tshirt\": \"t-shirt\",\n",
    "    \"polo\": \"polo\",\n",
    "    \"pants\": \"pants\",       \"trousers\": \"pants\",    \"slacks\": \"pants\",\n",
    "    \"skirt\": \"skirt\",       \"skirts\": \"skirt\",\n",
    "    \"jacket\": \"jacket\",     \"coat\": \"jacket\",\n",
    "    \"jeans\": \"jeans\",       \"denim\": \"jeans\",\n",
    "    \"shorts\": \"shorts\",\n",
    "    \"suit\": \"suit\",         \"blazer\": \"blazer\",\n",
    "    \"swimwear\": \"swimwear\", \"swim\": \"swimwear\",\n",
    "    \"underwear\": \"underwear\", \"bra\": \"underwear\",\n",
    "    \"hoodie\": \"hoodie\",     \"sweatshirt\": \"hoodie\",\n",
    "    \"knit\": \"knitwear\",     \"sweater\": \"knitwear\",  \"pullover\": \"knitwear\",\n",
    "    \"activewear\": \"activewear\", \"leggings\": \"activewear\",\n",
    "}\n",
    "\n",
    "print(\"✓ FIBER_RE and CLOTHING_TYPE_MAP ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16b7272d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ detect_clothing_type() ready\n"
     ]
    }
   ],
   "source": [
    "def detect_clothing_type(text: str, url: str = \"\") -> str:\n",
    "    \"\"\"Infer clothing category from page text + URL keywords.\"\"\"\n",
    "    combined = (text + \" \" + url).lower()\n",
    "    for kw, cat in CLOTHING_TYPE_MAP.items():\n",
    "        if kw in combined:\n",
    "            return cat\n",
    "    return \"unspecified\"\n",
    "\n",
    "\n",
    "print(\"✓ detect_clothing_type() ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "854f5428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ parse_fiber_composition() ready\n"
     ]
    }
   ],
   "source": [
    "def parse_fiber_composition(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract { fiber: percentage } from raw HTML page text.\n",
    "    Normalises synonyms (spandex→elastane, polyamide→nylon, etc.).\n",
    "    Rescales totals that are within ±15 % of 100.\n",
    "    \"\"\"\n",
    "    fibers: dict = {}\n",
    "    for m in FIBER_RE.finditer(text):\n",
    "        pct = float(m.group(1))\n",
    "        fib = (m.group(2).lower()\n",
    "               .replace(\"spandex\",   \"elastane\")\n",
    "               .replace(\"lycra\",     \"elastane\")\n",
    "               .replace(\"polyamide\", \"nylon\")\n",
    "               .replace(\"microfiber\",\"microfibre\"))\n",
    "        fibers[fib] = fibers.get(fib, 0.0) + pct\n",
    "    total = sum(fibers.values())\n",
    "    if total > 0 and abs(total - 100) <= 15:\n",
    "        fibers = {k: round(v / total * 100, 2) for k, v in fibers.items()}\n",
    "    return fibers\n",
    "\n",
    "\n",
    "print(\"✓ parse_fiber_composition() ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324411e1",
   "metadata": {},
   "source": [
    "### 1-B-2 · Page Scraper Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5701c923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ scrape_page() ready\n"
     ]
    }
   ],
   "source": [
    "def scrape_page(url: str, brand: str, source: str, country: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Fetch one collection/listing page, crawl up to 15 product detail links,\n",
    "    and extract fiber composition + clothing type from each.\n",
    "\n",
    "    Strategy:\n",
    "      1. GET the listing page.\n",
    "      2. Collect <a href> links containing product-path keywords\n",
    "         (/products/, /p/, /item/, /detail, /pd/).\n",
    "      3. Visit each product page; run parse_fiber_composition() on full text.\n",
    "      4. Skip pages that yield no fiber data.\n",
    "\n",
    "    Returns a list of record dicts (empty list on any hard failure).\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=SCRAPE_TIMEOUT)\n",
    "        if resp.status_code != 200:\n",
    "            return []\n",
    "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "        product_links: set = set()\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "            if any(kw in href.lower() for kw in\n",
    "                   [\"/products/\", \"/p/\", \"/item/\", \"/detail\", \"/pd/\"]):\n",
    "                if href.startswith(\"http\"):\n",
    "                    product_links.add(href)\n",
    "                elif href.startswith(\"/\"):\n",
    "                    product_links.add(urljoin(url, href))\n",
    "\n",
    "        pages_to_parse = list(product_links)[:15] if product_links else [url]\n",
    "\n",
    "        for purl in pages_to_parse:\n",
    "            try:\n",
    "                time.sleep(SCRAPE_DELAY + random.uniform(0, 0.5))\n",
    "                p_resp = requests.get(purl, headers=HEADERS, timeout=SCRAPE_TIMEOUT)\n",
    "                if p_resp.status_code != 200:\n",
    "                    continue\n",
    "                p_soup    = BeautifulSoup(p_resp.text, \"lxml\")\n",
    "                page_text = p_soup.get_text(\" \", strip=True)\n",
    "\n",
    "                fibers = parse_fiber_composition(page_text)\n",
    "                if not fibers:\n",
    "                    continue\n",
    "\n",
    "                name = \"\"\n",
    "                for sel in [\"h1\", \".product-title\", \".product-name\",\n",
    "                            '[data-testid=\"product-name\"]', \".pdp-title\"]:\n",
    "                    node = p_soup.select_one(sel)\n",
    "                    if node and node.get_text(strip=True):\n",
    "                        name = node.get_text(strip=True)[:120]\n",
    "                        break\n",
    "\n",
    "                # Capture a short snippet of raw fabric text for auditability\n",
    "                fab_text = \"\"\n",
    "                for m in FIBER_RE.finditer(page_text):\n",
    "                    start    = max(0, m.start() - 60)\n",
    "                    end      = min(len(page_text), m.end() + 60)\n",
    "                    fab_text = page_text[start:end].strip()[:200]\n",
    "                    break\n",
    "\n",
    "                records.append({\n",
    "                    \"brand\":               brand,\n",
    "                    \"product_name\":        name or brand,\n",
    "                    \"clothing_type\":       detect_clothing_type(page_text, purl),\n",
    "                    \"fabric_composition\":  fab_text,\n",
    "                    \"fiber_json\":          json.dumps(fibers),\n",
    "                    \"most_dominant_fiber\": max(fibers, key=fibers.get),\n",
    "                    \"source\":              source,\n",
    "                    \"country_of_brand\":    country,\n",
    "                    \"scraped_url\":         purl,\n",
    "                    \"scraped_at\":          datetime.datetime.utcnow().isoformat(),\n",
    "                    \"origin\":              \"live\",\n",
    "                })\n",
    "            except Exception:\n",
    "                continue\n",
    "    except Exception:\n",
    "        pass   # silently skip — seed catalog covers missing brands\n",
    "    return records\n",
    "\n",
    "\n",
    "print(\"✓ scrape_page() ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d890e9",
   "metadata": {},
   "source": [
    "### 1-B-3 · Collection URL Discovery\n",
    "\n",
    "`discover_collection_urls(root_url)` crawls the brand's homepage and returns\n",
    "all clothing-category / collection listing pages it finds in `<a href>` links —\n",
    "no hardcoded paths required. The run-scraper cell feeds these discovered URLs\n",
    "into `scrape_page()` instead of a single fixed URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46418436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ _COLLECTION_RE and _SKIP_RE ready\n"
     ]
    }
   ],
   "source": [
    "# ── Private regex constants for discover_collection_urls() ────────────────\n",
    "# Path segments that indicate a clothing collection / category page\n",
    "_COLLECTION_RE = re.compile(\n",
    "    r'/('\n",
    "    r'collections?|categories?|category|'\n",
    "    r'clothing|clothes|apparel|'\n",
    "    r'c/|browse/|shop/|products?/|'\n",
    "    r'women|mens?|ladies|girls?|boys?|kids?|'\n",
    "    r'tops?|shirts?|blouses?|dresses?|skirts?|'\n",
    "    r'pants?|trousers?|jeans?|bottoms?|'\n",
    "    r'jackets?|coats?|outerwear|knitwear|sweaters?|hoodies?|'\n",
    "    r'activewear|swimwear|underwear|lingerie'\n",
    "    r')',\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# Path segments that are definitely NOT product listings\n",
    "_SKIP_RE = re.compile(\n",
    "    r'/(cart|checkout|account|login|register|search|wishlist|'\n",
    "    r'faq|about|contact|blog|press|careers?|stores?|'\n",
    "    r'sustainability|privacy|returns?|size.?guide|gift)',\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "print(\"✓ _COLLECTION_RE and _SKIP_RE ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1467e9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ discover_collection_urls() ready\n"
     ]
    }
   ],
   "source": [
    "def discover_collection_urls(root_url: str, max_collections: int = 8) -> list[str]:\n",
    "    \"\"\"\n",
    "    Crawl root_url (brand homepage) and return up to max_collections\n",
    "    clothing-category / collection listing URLs found in <a href> links.\n",
    "\n",
    "    Strategy:\n",
    "      1. GET root_url.\n",
    "      2. Walk every <a href> — resolve relative URLs, keep same-domain only.\n",
    "      3. Filter by _COLLECTION_RE and exclude _SKIP_RE noise links.\n",
    "      4. Sort by number of clothing keywords in the path (most specific first).\n",
    "      5. Return up to max_collections unique URLs.\n",
    "      6. Fallback: if nothing is found, return [root_url] so scrape_page()\n",
    "         still has something to work with.\n",
    "    \"\"\"\n",
    "    parsed_root = urlparse(root_url)\n",
    "    base        = f\"{parsed_root.scheme}://{parsed_root.netloc}\"\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(root_url, headers=HEADERS, timeout=SCRAPE_TIMEOUT)\n",
    "        if resp.status_code != 200:\n",
    "            return [root_url]\n",
    "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "    except Exception:\n",
    "        return [root_url]\n",
    "\n",
    "    seen:       set  = set()\n",
    "    candidates: list = []\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        raw_href = a[\"href\"].strip()\n",
    "        if not raw_href or raw_href.startswith((\"#\", \"javascript\", \"mailto\")):\n",
    "            continue\n",
    "\n",
    "        # Resolve to absolute URL; strip query-string and trailing slash\n",
    "        full = (raw_href if raw_href.startswith(\"http\") else urljoin(base, raw_href))\n",
    "        full = full.split(\"?\")[0].split(\"#\")[0].rstrip(\"/\")\n",
    "\n",
    "        # Keep same domain only\n",
    "        if urlparse(full).netloc != parsed_root.netloc:\n",
    "            continue\n",
    "\n",
    "        path = urlparse(full).path.lower()\n",
    "\n",
    "        if _SKIP_RE.search(path):\n",
    "            continue\n",
    "        if not _COLLECTION_RE.search(path):\n",
    "            continue\n",
    "        if full in seen:\n",
    "            continue\n",
    "\n",
    "        seen.add(full)\n",
    "        candidates.append(full)\n",
    "\n",
    "    # Rank: paths that contain more clothing-type keywords score higher\n",
    "    _RANK_WORDS = {\n",
    "        \"tops\", \"shirts\", \"dresses\", \"clothing\", \"collections\",\n",
    "        \"women\", \"men\", \"ladies\", \"apparel\", \"clothes\",\n",
    "    }\n",
    "    candidates.sort(\n",
    "        key=lambda u: sum(w in u.lower() for w in _RANK_WORDS),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    result = candidates[:max_collections]\n",
    "    if not result:\n",
    "        result = [root_url]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"✓ discover_collection_urls() ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d5eab",
   "metadata": {},
   "source": [
    "### 1-B-4 · Run Live Scraper\n",
    "\n",
    "For each brand, `discover_collection_urls()` crawls the root homepage and\n",
    "returns up to 8 clothing-category pages. `scrape_page()` is then called on\n",
    "each discovered page to harvest product detail links and extract fiber data.  \n",
    "Brands whose sites require JavaScript or block requests will yield 0 rows.\n",
    "\n",
    "> **Testing mode:** Currently running with **2 Philippine + 2 multinational brands** (`TEST_BRANDS`).  \n",
    "> To run the full 50-brand scrape, replace `TEST_BRANDS` with `ALL_BRANDS` in the scraper cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5374a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping brands:   6%|▌         | 3/50 [08:55<2:41:30, 206.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Penshoppe                       87 products  (8 collection pages crawled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping brands:  64%|██████▍   | 32/50 [12:23<08:46, 29.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Mango                           11 products  (8 collection pages crawled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping brands:  92%|█████████▏| 46/50 [13:42<00:49, 12.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Columbia                         4 products  (8 collection pages crawled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping brands:  94%|█████████▍| 47/50 [14:43<01:20, 26.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Everlane                         5 products  (8 collection pages crawled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping brands: 100%|██████████| 50/50 [14:49<00:00, 17.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Live scrape total: 107 rows from 4 brands\n",
      "\n",
      "✓ df_catalog: 71 rows | 4 brands\n",
      "\n",
      "  Clothing-type distribution:\n",
      "    dress              58\n",
      "    shirt               6\n",
      "    t-shirt             5\n",
      "    jacket              2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ── 1-B-4  Run live scraper across brand root URLs ────────────────────────\n",
    "#\n",
    "# TEST_MODE   = True  → scrape 2 PH + 2 multinational brands only\n",
    "# TEST_MODE   = False → scrape all 50 brands (full production run)\n",
    "#\n",
    "TEST_MODE   = True\n",
    "TEST_BRANDS = PH_BRANDS[:2] + MULTINATIONAL_BRANDS[:2]\n",
    "RUN_BRANDS  = TEST_BRANDS if TEST_MODE else ALL_BRANDS\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(f\"⚠ TEST MODE — scraping {len(RUN_BRANDS)} brands \"\n",
    "          f\"({len(PH_BRANDS[:2])} PH + {len(MULTINATIONAL_BRANDS[:2])} multinational):\")\n",
    "    for b in RUN_BRANDS:\n",
    "        print(f\"  • {b[0]}  ({b[3]})\")\n",
    "    print()\n",
    "else:\n",
    "    print(f\"PRODUCTION MODE — scraping all {len(RUN_BRANDS)} brands\")\n",
    "    print()\n",
    "\n",
    "_SCHEMA = [\n",
    "    \"brand\", \"product_name\", \"clothing_type\", \"fabric_composition\",\n",
    "    \"fiber_json\", \"most_dominant_fiber\", \"source\", \"country_of_brand\",\n",
    "    \"scraped_url\", \"scraped_at\", \"origin\",\n",
    "]\n",
    "\n",
    "live_rows = []\n",
    "\n",
    "try:\n",
    "    for brand_name, root_url, source, country in tqdm(RUN_BRANDS, desc=\"Scraping brands\"):\n",
    "        collection_urls = discover_collection_urls(root_url)\n",
    "        brand_rows = []\n",
    "        for col_url in collection_urls:\n",
    "            rows = scrape_page(col_url, brand_name, source, country)\n",
    "            brand_rows.extend(rows)\n",
    "        live_rows.extend(brand_rows)\n",
    "        if brand_rows:\n",
    "            print(f\"  ✓ {brand_name:30s} {len(brand_rows):3d} products  \"\n",
    "                  f\"({len(collection_urls)} collection pages crawled)\")\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n⚠ Scrape interrupted — {len(live_rows)} rows collected so far will be saved.\")\n",
    "\n",
    "# ── Full 50-brand loop (kept for production use — set TEST_MODE = False above) ──\n",
    "# try:\n",
    "#     for brand_name, root_url, source, country in tqdm(ALL_BRANDS, desc=\"Scraping brands\"):\n",
    "#         collection_urls = discover_collection_urls(root_url)\n",
    "#         brand_rows = []\n",
    "#         for col_url in collection_urls:\n",
    "#             rows = scrape_page(col_url, brand_name, source, country)\n",
    "#             brand_rows.extend(rows)\n",
    "#         live_rows.extend(brand_rows)\n",
    "#         if brand_rows:\n",
    "#             print(f\"  ✓ {brand_name:30s} {len(brand_rows):3d} products  \"\n",
    "#                   f\"({len(collection_urls)} collection pages crawled)\")\n",
    "# except KeyboardInterrupt:\n",
    "#     print(f\"\\n⚠ Scrape interrupted — {len(live_rows)} rows collected so far will be saved.\")\n",
    "\n",
    "print(f\"\\n  Live scrape total: {len(live_rows)} rows from \"\n",
    "      f\"{len({r['brand'] for r in live_rows})} brands\")\n",
    "\n",
    "df_catalog = (\n",
    "    pd.DataFrame(live_rows, columns=_SCHEMA)\n",
    "    if live_rows\n",
    "    else pd.DataFrame(columns=_SCHEMA)\n",
    ")\n",
    "df_catalog = df_catalog.drop_duplicates(subset=[\"brand\", \"product_name\"])\n",
    "\n",
    "print(f\"\\n✓ df_catalog: {len(df_catalog):,} rows | {df_catalog['brand'].nunique()} brands\")\n",
    "print(f\"\\n  Clothing-type distribution:\")\n",
    "for ct, n in df_catalog[\"clothing_type\"].value_counts().head(10).items():\n",
    "    print(f\"    {ct:<16} {n:>4}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d12ee",
   "metadata": {},
   "source": [
    "---\n",
    "## 1-C · Biodegradability Tier Classification — BRAND_FIBER_LOOKUP\n",
    "\n",
    "### Regulatory References\n",
    "\n",
    "The biodegradability tiers applied to each product's fiber composition are\n",
    "derived from two authoritative sources:\n",
    "\n",
    "**1. EU Regulation 2024/1781 — Ecodesign for Sustainable Products (ESPR)**\n",
    "> *Regulation (EU) 2024/1781 of the European Parliament and of the Council,*\n",
    "> Official Journal of the European Union, 2024.\n",
    "> Annex I (textile product groups) and the accompanying Commission staff\n",
    "> working document on textile sustainability scoring methodology.\n",
    "> The regulation establishes minimum recycled-content and natural-fiber\n",
    "> thresholds for product sustainability labelling across EU member states.\n",
    "> Delegated acts specifying exact numeric thresholds for textile\n",
    "> biodegradability scoring are ongoing as of 2024–2026.\n",
    "\n",
    "**2. GOTS v6.0 — Global Organic Textile Standard**\n",
    "> *Global Organic Textile Standard, Version 6.0*, GOTS, 2020.\n",
    "> Establishes **≥ 85 % certified organic natural fibers** as the minimum\n",
    "> threshold for main-label GOTS certification — the origin of the 85 %\n",
    "> boundary used in this notebook.\n",
    "\n",
    "### Tier Mapping Applied in This Notebook\n",
    "\n",
    "| Tier | Bio-fiber share | Interpretation |\n",
    "|------|----------------|----------------|\n",
    "| **high** | ≥ 85 % | Predominantly natural / biodegradable — GOTS-aligned |\n",
    "| **medium** | 50 – 84 % | Mixed composition |\n",
    "| **low** | < 50 % | Synthetic-dominant |\n",
    "\n",
    "> ⚠ **Note:** The exact numeric thresholds in the ESPR delegated textile act\n",
    "> are still being finalised. The 85 / 50 split is an evidence-based\n",
    "> approximation aligned with GOTS v6.0 and the draft ESPR textile methodology.\n",
    "\n",
    "---\n",
    "\n",
    "### 1-C-1 · Bio-Share & Tier Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86d374a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BIO_FIBERS vocabulary loaded: 14 fiber types\n",
      "✓ bio_share() and biodeg_tier() ready\n"
     ]
    }
   ],
   "source": [
    "# ── 1-C-1  Bio-fiber vocabulary ─────────────────────────────────────────\n",
    "\n",
    "BIO_FIBERS = frozenset([\n",
    "    \"cotton\", \"linen\", \"hemp\", \"wool\", \"silk\",\n",
    "    \"bamboo\", \"tencel\", \"lyocell\", \"modal\",\n",
    "    \"cashmere\", \"viscose\", \"rayon\", \"acetate\", \"denim\",\n",
    "])\n",
    "\n",
    "\n",
    "def bio_share(fibers: dict) -> float:\n",
    "    \"\"\"Return the percentage of bio/natural fibers in a fiber-composition dict.\"\"\"\n",
    "    total = sum(fibers.values())\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    return round(sum(v for k, v in fibers.items() if k in BIO_FIBERS) / total * 100, 2)\n",
    "\n",
    "\n",
    "def biodeg_tier(bio_pct: float) -> str:\n",
    "    \"\"\"Map a bio-fiber percentage to an EU-Ecodesign-aligned biodegradability tier.\n",
    "\n",
    "    Thresholds:\n",
    "        ≥ 85 % → 'high'   (GOTS v6.0 main-label threshold)\n",
    "        ≥ 50 % → 'medium'\n",
    "        < 50 % → 'low'\n",
    "    \"\"\"\n",
    "    if bio_pct >= 85:\n",
    "        return \"high\"\n",
    "    if bio_pct >= 50:\n",
    "        return \"medium\"\n",
    "    return \"low\"\n",
    "\n",
    "\n",
    "print(\"✓ BIO_FIBERS vocabulary loaded:\", len(BIO_FIBERS), \"fiber types\")\n",
    "print(\"✓ bio_share() and biodeg_tier() ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7274fdf",
   "metadata": {},
   "source": [
    "### 1-C-2 · Annotate Product Catalog\n",
    "\n",
    "Parse the `fiber_json` column of `df_catalog` into Python dicts, then compute\n",
    "`fs_bio_share` (% bio-fiber) and `fs_biodeg_tier` for every row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b816e239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Annotated 71 rows\n",
      "\n",
      "  Biodegradability tier distribution (EU Ecodesign 2024/1781 / GOTS v6.0):\n",
      "    low         32  (45.1 %)\n",
      "    medium      20  (28.2 %)\n",
      "    high        19  (26.8 %)\n",
      "\n",
      "  Sample rows:\n",
      "    brand clothing_type most_dominant_fiber  fs_bio_share fs_biodeg_tier     source\n",
      "Penshoppe         dress           polyester          35.0            low philippine\n",
      "Penshoppe         dress              cotton         100.0           high philippine\n",
      "Penshoppe         dress           polyester          35.0            low philippine\n",
      "Penshoppe         dress           polyester          35.0            low philippine\n",
      "Penshoppe         dress           polyester           0.0            low philippine\n",
      "Penshoppe         dress              cotton         100.0           high philippine\n",
      "Penshoppe         dress           polyester           0.0            low philippine\n",
      "Penshoppe         dress           polyester           0.0            low philippine\n"
     ]
    }
   ],
   "source": [
    "# ── §1-C-2  Parse fiber_json → annotate df_catalog ───────────────────────\n",
    "\n",
    "df_catalog[\"fiber_dict\"] = df_catalog[\"fiber_json\"].apply(\n",
    "    lambda x: json.loads(x) if isinstance(x, str) else {}\n",
    ")\n",
    "df_catalog[\"fs_bio_share\"]   = df_catalog[\"fiber_dict\"].apply(bio_share)\n",
    "df_catalog[\"fs_biodeg_tier\"] = df_catalog[\"fs_bio_share\"].apply(biodeg_tier)\n",
    "\n",
    "print(f\"✓ Annotated {len(df_catalog):,} rows\")\n",
    "print(\"\\n  Biodegradability tier distribution (EU Ecodesign 2024/1781 / GOTS v6.0):\")\n",
    "for tier, cnt in df_catalog[\"fs_biodeg_tier\"].value_counts().items():\n",
    "    print(f\"    {tier:<8} {cnt:>5}  ({cnt / len(df_catalog) * 100:.1f} %)\")\n",
    "\n",
    "print(\"\\n  Sample rows:\")\n",
    "print(df_catalog[[\"brand\", \"clothing_type\", \"most_dominant_fiber\",\n",
    "                   \"fs_bio_share\", \"fs_biodeg_tier\", \"source\"]].head(8).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a2890",
   "metadata": {},
   "source": [
    "### 1-C-3 · Aggregate Per-Brand Median Profile → BRAND_FIBER_LOOKUP\n",
    "\n",
    "For each brand, compute the **median fiber share** across all scraped products.\n",
    "Fibers contributing < 2 % at the median are dropped to keep the profile clean.\n",
    "The resulting dict `BRAND_FIBER_LOOKUP[brand]` holds:\n",
    "`fibers`, `bio_share`, `biodeg_tier`, `item_count`, `source`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e9fc843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BRAND_FIBER_LOOKUP built: 4 brands\n",
      "\n",
      "  Sample entries:\n",
      "    columbia               tier=low      bio=  0.0%  items=  4  fibers={'nylon': 38.3, 'elastane': 5.2, 'polyester': 56.5}\n",
      "    everlane               tier=medium   bio= 67.6%  items=  5  fibers={'alpaca': 32.4, 'wool': 8.8, 'tencel': 35.3, 'cotton': 23.5}\n",
      "    mango                  tier=high     bio=100.0%  items= 11  fibers={'cotton': 100.0}\n",
      "    penshoppe              tier=low      bio= 49.6%  items= 51  fibers={'polyester': 23.9, 'cotton': 22.1, 'nylon': 26.5, 'viscose': 27.6}\n"
     ]
    }
   ],
   "source": [
    "# ── 1-C-3  Per-brand median fiber profile ───────────────────────────────\n",
    "\n",
    "_brand_records = []\n",
    "for brand, grp in df_catalog[df_catalog[\"fiber_dict\"].apply(bool)].groupby(\"brand\"):\n",
    "    agg: dict = {}\n",
    "    for fdict in grp[\"fiber_dict\"]:\n",
    "        for k, v in fdict.items():\n",
    "            agg.setdefault(k, []).append(v)\n",
    "\n",
    "    # median share per fiber; drop fibers with median ≤ 2 %\n",
    "    brand_fiber = {\n",
    "        k: round(float(np.median(v)), 1)\n",
    "        for k, v in agg.items()\n",
    "        if np.median(v) > 2\n",
    "    }\n",
    "\n",
    "    # re-normalise to 100 %\n",
    "    total = sum(brand_fiber.values())\n",
    "    if total > 0:\n",
    "        brand_fiber = {k: round(v / total * 100, 1) for k, v in brand_fiber.items()}\n",
    "\n",
    "    _brand_records.append({\n",
    "        \"brand\":       brand.lower().strip(),\n",
    "        \"fibers\":      brand_fiber,\n",
    "        \"bio_share\":   bio_share(brand_fiber),\n",
    "        \"biodeg_tier\": biodeg_tier(bio_share(brand_fiber)),\n",
    "        \"item_count\":  len(grp),\n",
    "        \"source\":      grp[\"source\"].iloc[0],\n",
    "    })\n",
    "\n",
    "BRAND_FIBER_LOOKUP: dict = {r[\"brand\"]: r for r in _brand_records}\n",
    "\n",
    "print(f\"✓ BRAND_FIBER_LOOKUP built: {len(BRAND_FIBER_LOOKUP)} brands\")\n",
    "print(\"\\n  Sample entries:\")\n",
    "for brand, rec in list(BRAND_FIBER_LOOKUP.items())[:4]:\n",
    "    print(f\"    {brand:<22} tier={rec['biodeg_tier']:<8} bio={rec['bio_share']:5.1f}%  \"\n",
    "          f\"items={rec['item_count']:>3}  fibers={rec['fibers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcbee50",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ab5d5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 20260224-212202-webscraped_catalog.csv  →  /Users/gyalm/Desktop/WeaveForward_FiberClassificationML/data/webscraped_data/20260224-212202-webscraped_catalog.csv\n",
      "✓ webscraped_catalog.csv (stable alias)  →  /Users/gyalm/Desktop/WeaveForward_FiberClassificationML/data/webscraped_data/webscraped_catalog.csv\n",
      "  71 rows | 4 brands | 13 columns\n"
     ]
    }
   ],
   "source": [
    "# ── Save webscraped catalog CSV — timestamped ─────────────────────────────\n",
    "_ts              = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "catalog_csv      = WEB_DIR / f\"{_ts}-webscraped_catalog.csv\"\n",
    "catalog_csv_stable = WEB_DIR / \"webscraped_catalog.csv\"   # stable alias\n",
    "\n",
    "_save_cols = [\n",
    "    \"brand\", \"product_name\", \"clothing_type\", \"fabric_composition\",\n",
    "    \"fiber_json\", \"most_dominant_fiber\", \"fs_bio_share\", \"fs_biodeg_tier\",\n",
    "    \"source\", \"country_of_brand\", \"scraped_url\", \"scraped_at\", \"origin\",\n",
    "]\n",
    "df_catalog[_save_cols].to_csv(catalog_csv,        index=False, encoding=\"utf-8\")\n",
    "df_catalog[_save_cols].to_csv(catalog_csv_stable, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✓ {catalog_csv.name}  →  {catalog_csv}\")\n",
    "print(f\"✓ webscraped_catalog.csv (stable alias)  →  {catalog_csv_stable}\")\n",
    "print(f\"  {len(df_catalog):,} rows | {df_catalog['brand'].nunique()} brands | \"\n",
    "      f\"{len(_save_cols)} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cacace",
   "metadata": {},
   "source": [
    "### Save BRAND_FIBER_LOOKUP\n",
    "\n",
    "Writes `brand_fiber_lookup.json` to `data/processed/` for backward compatibility\n",
    "with the main classification notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5326e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save BRAND_FIBER_LOOKUP JSON (timestamped + stable alias) ─────────────\n",
    "lookup_path        = PROC_DIR / f\"{_ts}-brand_fiber_lookup.json\"\n",
    "lookup_path_stable = PROC_DIR / \"brand_fiber_lookup.json\"\n",
    "\n",
    "for path in [lookup_path, lookup_path_stable]:\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(BRAND_FIBER_LOOKUP, f, indent=2)\n",
    "\n",
    "print(f\"✓ {lookup_path.name}  →  {lookup_path}\")\n",
    "print(f\"✓ brand_fiber_lookup.json (stable alias)  →  {lookup_path_stable}\")\n",
    "print(f\"  {len(BRAND_FIBER_LOOKUP):,} brand entries\")\n",
    "\n",
    "print(f\"\\n── Webscraper extraction complete — run ID: {_ts} ─────────────────\")\n",
    "print(f\"   Next: load  data/webscraped_data/{catalog_csv.name}\")\n",
    "print(f\"         into  weaveforward_fiber_recommendation.ipynb\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
